Data Modeling with Postgres: Song Library

This is an ETL Pipeline designed for a startup named Sparkify with the purpose of analyzing the data they have been collecting on songs and user activity on their new music streaming app. This project creates a star-schema database from a directory of JSON logs on user activity on the app  and a JSON  directory with metadata on the songs.

Project Datasets
Million Song Dataset: real data containing metadata about a song and it's respective artist. JSON files are partitioned into separate directories by the first three letters of the song's track ID.
Event simulator: User actvitiy log files based on songs in the dataset above. JSON files are partitioned by year and month.

Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. 

Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.
The log files in the dataset are partitioned by year and month. 

Project Description
In this project an ETL pipeline is built using Python. Fact and dimension tables are created  that follow a star schema structure.The ETL pipeline transfers data from files in two local directories into these tables in Postgres using Python and SQL.

Fact Table
songplays - records in log data associated with song plays i.e. records with page NextSong songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

Dimension Tables
users - users in the app user_id, first_name, last_name, gender, level
songs - songs in music database song_id, title, artist_id, year, duration
artists - artists in music database artist_id, name, location, lattitude, longitude
time - timestamps of records in songplays broken down into specific units start_time, hour, day, week, month, year, weekday

Database creation script

create_tables.py is a script for creating and recreating the target database  provided for easy editions. Just run ´python 

sql_queries.py - SQL queries consists of drop create and insert queries imported into create_table.py and etl.py and etl.ipynb

test.ipynb - Test file to see if data was loaded properly

etl.ipynb - A Jupiter notebook is provided to document and demonstrate the ETL pipeline, 

etl.py -An ETL script automatically loops through the logs and songs directories, transforms the data using Python/Pandas, and inserts it on the star-schema 

Song Files
    • Get json filepath using get_files function
    • Read json file into pandas dataframe
    • Transform dataframe's array values into a list
    • Insert list into appropriate database table using psycopg2 (PostgreSQL Python wrapper)
Log Files
    • Get json filepath using get_files function
    • Read json file into pandas dataframe
    • Separate time and user columns from the dataframe into separate dataframes
    • Transform dataframes' array values into separate list
    • Insert lists into appropriate database table using psycopg2 (PostgreSQL Python wrapper)
    • For the songplay table,the  SQL query joins artists and songs tables on artist_id and inserts necessary columns 
     The necessary columns from the log dataframe are also inserted within the same query.
